# app.py - Streamlit + LangChain ì˜ˆì œ with Chat History í‘œì‹œ (ìˆ˜ì •ë¨)
import os
import streamlit as st
from dotenv import load_dotenv
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, AIMessage

# Agent ê´€ë ¨ ëª¨ë“ˆ
from langchain import hub
from langchain.agents import AgentExecutor, create_openai_tools_agent, load_tools
from langchain_community.callbacks import StreamlitCallbackHandler

# Memory ê´€ë ¨ ëª¨ë“ˆ
from langchain.memory import ConversationBufferMemory

# ğŸ“Œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ğŸ“Œ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ Session Stateì— ì €ì¥
if "chat_history" not in st.session_state:
    st.session_state.chat_history = StreamlitChatMessageHistory()

# ğŸ“Œ Agent ìƒì„± í•¨ìˆ˜
def create_agent_chain(history):
    chat = ChatOpenAI(
        model_name=os.getenv("OPENAI_API_MODEL", "gpt-3.5-turbo"),
        temperature=float(os.getenv("OPENAI_API_TEMPERATURE", 0.5)),
        max_tokens=500
    )

    # ğŸ”§ ë„êµ¬ ë¡œë“œ
    tools = load_tools(["ddg-search", "wikipedia"])

    # ğŸ”§ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    prompt = hub.pull("hwchase17/openai-tools-agent")

    # ğŸ“ ConversationBufferMemory ì´ˆê¸°í™”
    memory = ConversationBufferMemory(
        chat_memory=history,
        memory_key='chat_history',
        return_messages=True,
        output_key=None
    )

    # ğŸ› ï¸ Agent ìƒì„±
    agent = create_openai_tools_agent(chat, tools, prompt)

    # ğŸš€ Agent Executor ìƒì„±
    return AgentExecutor.from_agent_and_tools(
        agent=agent,
        tools=tools,
        memory=memory,
        verbose=True,
        return_intermediate_steps=False
    )

# ğŸ“Œ Streamlit ì œëª© ë° ì„¤ëª…
st.title("ğŸš€ AWS EC2 + LangChain Agent Chatbot")
st.write("LangChain Agentsë¥¼ í™œìš©í•œ Streamlit ì±—ë´‡ì…ë‹ˆë‹¤. ğŸ‰")

# ğŸ’¬ ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶œë ¥
st.subheader("ğŸ’¬ ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬")
for message in st.session_state.chat_history.messages[:-1]:  # ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì œì™¸
    if isinstance(message, HumanMessage):
        with st.chat_message("user"):
            # st.markdown(message.content)
    elif isinstance(message, AIMessage):
        with st.chat_message("assistant"):
            # st.markdown(message.content)

# ğŸŸ¡ ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
prompt = st.chat_input("What's up?")

if prompt:
    # ğŸ—¨ï¸ ì‚¬ìš©ì ë©”ì‹œì§€ ì¦‰ì‹œ í‘œì‹œ
    with st.chat_message("user"):
        st.markdown(prompt)

    # ğŸ¤– AI ì‘ë‹µ ìƒì„± ë° ì¶œë ¥
    with st.chat_message("assistant"):
        callback = StreamlitCallbackHandler(st.container())  # ì½œë°± í•¸ë“¤ëŸ¬ ì¶”ê°€
        agent_chain = create_agent_chain(st.session_state.chat_history)

        try:
            response = agent_chain.invoke({"input": prompt})
            output = response.get("output", "No response generated.")

            # ğŸ“ Session State íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            st.session_state.chat_history.add_user_message(prompt)
            st.session_state.chat_history.add_ai_message(output)

            # AI ì‘ë‹µ ì¦‰ì‹œ í‘œì‹œ
            st.markdown(output)

        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

# ğŸ”„ ğŸ“ ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¦‰ì‹œ ì—…ë°ì´íŠ¸
st.divider()
st.subheader("ğŸ“ ì „ì²´ ëŒ€í™” ë‚´ì—­")
for message in st.session_state.chat_history.messages:
    if isinstance(message, HumanMessage):
        st.markdown(f"ğŸ‘¤ **ì‚¬ìš©ì:** {message.content}")
    elif isinstance(message, AIMessage):
        st.markdown(f"ğŸ¤– **AI:** {message.content}")